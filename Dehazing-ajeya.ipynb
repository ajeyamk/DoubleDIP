{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dehazing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lTZL-ZitzJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_io.py\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# import skvideo.io\n",
        "\n",
        "output_path_str = \"/content/drive/My Drive/Deep-learning/double-dip/output/\"\n",
        "matplotlib.use('agg')\n",
        "\n",
        "\n",
        "def crop_image(img, d=32):\n",
        "    \"\"\"\n",
        "    Make dimensions divisible by d\n",
        "    :param pil img:\n",
        "    :param d:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    new_size = (img.size[0] - img.size[0] % d,\n",
        "                img.size[1] - img.size[1] % d)\n",
        "\n",
        "    bbox = [\n",
        "        int((img.size[0] - new_size[0]) / 2),\n",
        "        int((img.size[1] - new_size[1]) / 2),\n",
        "        int((img.size[0] + new_size[0]) / 2),\n",
        "        int((img.size[1] + new_size[1]) / 2),\n",
        "    ]\n",
        "\n",
        "    img_cropped = img.crop(bbox)\n",
        "    return img_cropped\n",
        "\n",
        "\n",
        "def crop_np_image(img_np, d=32):\n",
        "    return torch_to_np(crop_torch_image(np_to_torch(img_np), d))\n",
        "\n",
        "\n",
        "def crop_torch_image(img, d=32):\n",
        "    \"\"\"\n",
        "    Make dimensions divisible by d\n",
        "    image is [1, 3, W, H] or [3, W, H]\n",
        "    :param pil img:\n",
        "    :param d:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_size = (img.shape[-2] - img.shape[-2] % d,\n",
        "                img.shape[-1] - img.shape[-1] % d)\n",
        "    pad = ((img.shape[-2] - new_size[-2]) // 2, (img.shape[-1] - new_size[-1]) // 2)\n",
        "\n",
        "    if len(img.shape) == 4:\n",
        "        return img[:, :, pad[-2]: pad[-2] + new_size[-2], pad[-1]: pad[-1] + new_size[-1]]\n",
        "    assert len(img.shape) == 3\n",
        "    return img[:, pad[-2]: pad[-2] + new_size[-2], pad[-1]: pad[-1] + new_size[-1]]\n",
        "\n",
        "\n",
        "def get_params(opt_over, net, net_input, downsampler=None):\n",
        "    \"\"\"\n",
        "    Returns parameters that we want to optimize over.\n",
        "    :param opt_over: comma separated list, e.g. \"net,input\" or \"net\"\n",
        "    :param net: network\n",
        "    :param net_input: torch.Tensor that stores input `z`\n",
        "    :param downsampler:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    opt_over_list = opt_over.split(',')\n",
        "    params = []\n",
        "\n",
        "    for opt in opt_over_list:\n",
        "\n",
        "        if opt == 'net':\n",
        "            params += [x for x in net.parameters()]\n",
        "        elif opt == 'down':\n",
        "            assert downsampler is not None\n",
        "            params = [x for x in downsampler.parameters()]\n",
        "        elif opt == 'input':\n",
        "            net_input.requires_grad = True\n",
        "            params += [net_input]\n",
        "        else:\n",
        "            assert False, 'what is it?'\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_image_grid(images_np, nrow=8):\n",
        "    \"\"\"\n",
        "    Creates a grid from a list of images by concatenating them.\n",
        "    :param images_np:\n",
        "    :param nrow:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    images_torch = [torch.from_numpy(x).type(torch.FloatTensor) for x in images_np]\n",
        "    torch_grid = torchvision.utils.make_grid(images_torch, nrow)\n",
        "\n",
        "    return torch_grid.numpy()\n",
        "\n",
        "\n",
        "def plot_image_grid(name, images_np, interpolation='lanczos', output_path=output_path_str):\n",
        "    \"\"\"\n",
        "    Draws images in a grid\n",
        "    Args:\n",
        "        images_np: list of images, each image is np.array of size 3xHxW or 1xHxW\n",
        "        nrow: how many images will be in one row\n",
        "        interpolation: interpolation used in plt.imshow\n",
        "    \"\"\"\n",
        "    assert len(images_np) == 2 \n",
        "    n_channels = max(x.shape[0] for x in images_np)\n",
        "    assert (n_channels == 3) or (n_channels == 1), \"images should have 1 or 3 channels\"\n",
        "\n",
        "    images_np = [x if (x.shape[0] == n_channels) else np.concatenate([x, x, x], axis=0) for x in images_np]\n",
        "\n",
        "    grid = get_image_grid(images_np, 2)\n",
        "\n",
        "    if images_np[0].shape[0] == 1:\n",
        "        plt.imshow(grid[0], cmap='gray', interpolation=interpolation)\n",
        "    else:\n",
        "        plt.imshow(grid.transpose(1, 2, 0), interpolation=interpolation)\n",
        "\n",
        "    plt.savefig(output_path + \"{}.png\".format(name))\n",
        "\n",
        "\n",
        "def save_image(name, image_np, output_path=output_path_str):\n",
        "    p = np_to_pil(image_np)\n",
        "    p.save(output_path + \"{}.jpg\".format(name))\n",
        "\n",
        "def video_to_images(file_name, name):\n",
        "    video = prepare_video(file_name)\n",
        "    for i, f in enumerate(video):\n",
        "        save_image(name + \"_{0:03d}\".format(i), f)\n",
        "\n",
        "def images_to_video(images_dir ,name, gray=True):\n",
        "    num = len(glob.glob(images_dir +\"/*.jpg\"))\n",
        "    c = []\n",
        "    for i in range(num):\n",
        "        if gray:\n",
        "            img = prepare_gray_image(images_dir + \"/\"+  name +\"_{}.jpg\".format(i))\n",
        "        else:\n",
        "            img = prepare_image(images_dir + \"/\"+name+\"_{}.jpg\".format(i))\n",
        "        print(img.shape)\n",
        "        c.append(img)\n",
        "    save_video(name, np.array(c))\n",
        "\n",
        "def save_heatmap(name, image_np):\n",
        "    cmap = plt.get_cmap('jet')\n",
        "\n",
        "    rgba_img = cmap(image_np)\n",
        "    rgb_img = np.delete(rgba_img, 3, 2)\n",
        "    save_image(name, rgb_img.transpose(2, 0, 1))\n",
        "\n",
        "\n",
        "def save_graph(name, graph_list, output_path=output_path_str):\n",
        "    plt.clf()\n",
        "    plt.plot(graph_list)\n",
        "    plt.savefig(output_path + name + \".png\")\n",
        "\n",
        "\n",
        "def create_augmentations(np_image):\n",
        "    \"\"\"\n",
        "    convention: original, left, upside-down, right, rot1, rot2, rot3\n",
        "    :param np_image:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    aug = [np_image.copy(), np.rot90(np_image, 1, (1, 2)).copy(),\n",
        "           np.rot90(np_image, 2, (1, 2)).copy(), np.rot90(np_image, 3, (1, 2)).copy()]\n",
        "    flipped = np_image[:,::-1, :].copy()\n",
        "    aug += [flipped.copy(), np.rot90(flipped, 1, (1, 2)).copy(), np.rot90(flipped, 2, (1, 2)).copy(), np.rot90(flipped, 3, (1, 2)).copy()]\n",
        "    return aug\n",
        "\n",
        "\n",
        "def create_video_augmentations(np_video):\n",
        "    \"\"\"\n",
        "        convention: original, left, upside-down, right, rot1, rot2, rot3\n",
        "        :param np_video:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    aug = [np_video.copy(), np.rot90(np_video, 1, (2, 3)).copy(),\n",
        "           np.rot90(np_video, 2, (2, 3)).copy(), np.rot90(np_video, 3, (2, 3)).copy()]\n",
        "    flipped = np_video[:, :, ::-1, :].copy()\n",
        "    aug += [flipped.copy(), np.rot90(flipped, 1, (2, 3)).copy(), np.rot90(flipped, 2, (2, 3)).copy(),\n",
        "            np.rot90(flipped, 3, (2, 3)).copy()]\n",
        "    return aug\n",
        "\n",
        "\n",
        "def save_graphs(name, graph_dict, output_path=output_path_str):\n",
        "    \"\"\"\n",
        "    :param name:\n",
        "    :param dict graph_dict: a dict from the name of the list to the list itself.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots()\n",
        "    for k, v in graph_dict.items():\n",
        "        ax.plot(v, label=k)\n",
        "        # ax.semilogy(v, label=k)\n",
        "    ax.set_xlabel('iterations')\n",
        "    # ax.set_ylabel(name)\n",
        "    ax.set_ylabel('MSE-loss')\n",
        "    # ax.set_ylabel('PSNR')\n",
        "    plt.legend()\n",
        "    plt.savefig(output_path + name + \".png\")\n",
        "\n",
        "\n",
        "def load(path):\n",
        "    \"\"\"Load PIL image.\"\"\"\n",
        "    img = Image.open(path)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_image(path, imsize=-1):\n",
        "    \"\"\"Load an image and resize to a cpecific size.\n",
        "    Args:\n",
        "        path: path to image\n",
        "        imsize: tuple or scalar with dimensions; -1 for `no resize`\n",
        "    \"\"\"\n",
        "    img = load(path)\n",
        "\n",
        "    if isinstance(imsize, int):\n",
        "        imsize = (imsize, imsize)\n",
        "\n",
        "    if imsize[0] != -1 and img.size != imsize:\n",
        "        if imsize[0] > img.size[0]:\n",
        "            img = img.resize(imsize, Image.BICUBIC)\n",
        "        else:\n",
        "            img = img.resize(imsize, Image.ANTIALIAS)\n",
        "\n",
        "    img_np = pil_to_np(img)\n",
        "\n",
        "    return img, img_np\n",
        "\n",
        "\n",
        "def prepare_image(file_name):\n",
        "    \"\"\"\n",
        "    loads makes it divisible\n",
        "    :param file_name:\n",
        "    :return: the numpy representation of the image\n",
        "    \"\"\"\n",
        "    img_pil = crop_image(get_image(file_name, -1)[0], d=32)\n",
        "    return pil_to_np(img_pil)\n",
        "\n",
        "\n",
        "def prepare_video(file_name, folder=\"output/\"):\n",
        "    data = skvideo.io.vread(folder + file_name)\n",
        "    return crop_torch_image(data.transpose(0, 3, 1, 2).astype(np.float32) / 255.)[:35]\n",
        "\n",
        "\n",
        "def save_video(name, video_np, output_path=output_path_str):\n",
        "    outputdata = video_np * 255\n",
        "    outputdata = outputdata.astype(np.uint8)\n",
        "    skvideo.io.vwrite(output_path + \"{}.mp4\".format(name), outputdata.transpose(0, 2, 3, 1))\n",
        "\n",
        "\n",
        "def prepare_gray_image(file_name):\n",
        "    img = prepare_image(file_name)\n",
        "    return np.array([np.mean(img, axis=0)])\n",
        "\n",
        "\n",
        "def pil_to_np(img_PIL, with_transpose=True):\n",
        "    \"\"\"\n",
        "    Converts image in PIL format to np.array.\n",
        "    From W x H x C [0...255] to C x W x H [0..1]\n",
        "    \"\"\"\n",
        "    ar = np.array(img_PIL)\n",
        "    if len(ar.shape) == 3 and ar.shape[-1] == 4:\n",
        "        ar = ar[:, :, :3]\n",
        "        # this is alpha channel\n",
        "    if with_transpose:\n",
        "        if len(ar.shape) == 3:\n",
        "            ar = ar.transpose(2, 0, 1)\n",
        "        else:\n",
        "            ar = ar[None, ...]\n",
        "\n",
        "    return ar.astype(np.float32) / 255.\n",
        "\n",
        "\n",
        "def median(img_np_list):\n",
        "    \"\"\"\n",
        "    assumes C x W x H [0..1]\n",
        "    :param img_np_list:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(img_np_list) > 0\n",
        "    l = len(img_np_list)\n",
        "    shape = img_np_list[0].shape\n",
        "    result = np.zeros(shape)\n",
        "    for c in range(shape[0]):\n",
        "        for w in range(shape[1]):\n",
        "            for h in range(shape[2]):\n",
        "                result[c, w, h] = sorted(i[c, w, h] for i in img_np_list)[l//2]\n",
        "    return result\n",
        "\n",
        "\n",
        "def average(img_np_list):\n",
        "    \"\"\"\n",
        "    assumes C x W x H [0..1]\n",
        "    :param img_np_list:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(img_np_list) > 0\n",
        "    l = len(img_np_list)\n",
        "    shape = img_np_list[0].shape\n",
        "    result = np.zeros(shape)\n",
        "    for i in img_np_list:\n",
        "        result += i\n",
        "    return result / l\n",
        "\n",
        "\n",
        "def np_to_pil(img_np):\n",
        "    \"\"\"\n",
        "    Converts image in np.array format to PIL image.\n",
        "    From C x W x H [0..1] to  W x H x C [0...255]\n",
        "    :param img_np:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    ar = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    if img_np.shape[0] == 1:\n",
        "        ar = ar[0]\n",
        "    else:\n",
        "        assert img_np.shape[0] == 3, img_np.shape\n",
        "        ar = ar.transpose(1, 2, 0)\n",
        "\n",
        "    return Image.fromarray(ar)\n",
        "\n",
        "\n",
        "def np_to_torch(img_np):\n",
        "    \"\"\"\n",
        "    Converts image in numpy.array to torch.Tensor.\n",
        "    From C x W x H [0..1] to  C x W x H [0..1]\n",
        "    :param img_np:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return torch.from_numpy(img_np)[None, :]\n",
        "\n",
        "\n",
        "def torch_to_np(img_var):\n",
        "    \"\"\"\n",
        "    Converts an image in torch.Tensor format to np.array.\n",
        "    From 1 x C x W x H [0..1] to  C x W x H [0..1]\n",
        "    :param img_var:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return img_var.detach().cpu().numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TIx-Nr9kh7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b3133fad-bb15-4fb9-f175-d297615b57e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLMdyEThuRR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imresize.py\n",
        "import numpy as np\n",
        "from scipy.ndimage import filters, measurements, interpolation\n",
        "from math import pi\n",
        "\n",
        "\n",
        "def imresize(im, scale_factor=None, output_shape=None, kernel=None, antialiasing=True, kernel_shift_flag=False):\n",
        "    # First standardize values and fill missing arguments (if needed) by deriving scale from output shape or vice versa\n",
        "    scale_factor, output_shape = fix_scale_and_size(im.shape, output_shape, scale_factor)\n",
        "\n",
        "    # For a given numeric kernel case, just do convolution and sub-sampling (downscaling only)\n",
        "    if type(kernel) == np.ndarray and scale_factor[0] <= 1:\n",
        "        return numeric_kernel(im, kernel, scale_factor, output_shape, kernel_shift_flag)\n",
        "\n",
        "    # Choose interpolation method, each method has the matching kernel size\n",
        "    method, kernel_width = {\n",
        "        \"cubic\": (cubic, 4.0),\n",
        "        \"lanczos2\": (lanczos2, 4.0),\n",
        "        \"lanczos3\": (lanczos3, 6.0),\n",
        "        \"box\": (box, 1.0),\n",
        "        \"linear\": (linear, 2.0),\n",
        "        None: (cubic, 4.0)  # set default interpolation method as cubic\n",
        "    }.get(kernel)\n",
        "\n",
        "    # Antialiasing is only used when downscaling\n",
        "    antialiasing *= (scale_factor[0] < 1)\n",
        "\n",
        "    # Sort indices of dimensions according to scale of each dimension. since we are going dim by dim this is efficient\n",
        "    sorted_dims = np.argsort(np.array(scale_factor)).tolist()\n",
        "\n",
        "    # Iterate over dimensions to calculate local weights for resizing and resize each time in one direction\n",
        "    out_im = np.copy(im)\n",
        "    for dim in sorted_dims:\n",
        "        # No point doing calculations for scale-factor 1. nothing will happen anyway\n",
        "        if scale_factor[dim] == 1.0:\n",
        "            continue\n",
        "\n",
        "        # for each coordinate (along 1 dim), calculate which coordinates in the input image affect its result and the\n",
        "        # weights that multiply the values there to get its result.\n",
        "        weights, field_of_view = contributions(im.shape[dim], output_shape[dim], scale_factor[dim],\n",
        "                                               method, kernel_width, antialiasing)\n",
        "\n",
        "        # Use the affecting position values and the set of weights to calculate the result of resizing along this 1 dim\n",
        "        out_im = resize_along_dim(out_im, dim, weights, field_of_view)\n",
        "\n",
        "    return out_im\n",
        "\n",
        "\n",
        "def fix_scale_and_size(input_shape, output_shape, scale_factor):\n",
        "    # First fixing the scale-factor (if given) to be standardized the function expects (a list of scale factors in the\n",
        "    # same size as the number of input dimensions)\n",
        "    if scale_factor is not None:\n",
        "        # By default, if scale-factor is a scalar we assume 2d resizing and duplicate it.\n",
        "        if np.isscalar(scale_factor):\n",
        "            scale_factor = [scale_factor, scale_factor]\n",
        "\n",
        "        # We extend the size of scale-factor list to the size of the input by assigning 1 to all the unspecified scales\n",
        "        scale_factor = list(scale_factor)\n",
        "        scale_factor.extend([1] * (len(input_shape) - len(scale_factor)))\n",
        "\n",
        "    # Fixing output-shape (if given): extending it to the size of the input-shape, by assigning the original input-size\n",
        "    # to all the unspecified dimensions\n",
        "    if output_shape is not None:\n",
        "        output_shape = list(np.uint(np.array(output_shape))) + list(input_shape[len(output_shape):])\n",
        "\n",
        "    # Dealing with the case of non-give scale-factor, calculating according to output-shape. note that this is\n",
        "    # sub-optimal, because there can be different scales to the same output-shape.\n",
        "    if scale_factor is None:\n",
        "        scale_factor = 1.0 * np.array(output_shape) / np.array(input_shape)\n",
        "\n",
        "    # Dealing with missing output-shape. calculating according to scale-factor\n",
        "    if output_shape is None:\n",
        "        output_shape = np.uint(np.ceil(np.array(input_shape) * np.array(scale_factor)))\n",
        "\n",
        "    return scale_factor, output_shape\n",
        "\n",
        "\n",
        "def contributions(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n",
        "    # This function calculates a set of 'filters' and a set of field_of_view that will later on be applied\n",
        "    # such that each position from the field_of_view will be multiplied with a matching filter from the\n",
        "    # 'weights' based on the interpolation method and the distance of the sub-pixel location from the pixel centers\n",
        "    # around it. This is only done for one dimension of the image.\n",
        "\n",
        "    # When anti-aliasing is activated (default and only for downscaling) the receptive field is stretched to size of\n",
        "    # 1/sf. this means filtering is more 'low-pass filter'.\n",
        "    fixed_kernel = (lambda arg: scale * kernel(scale * arg)) if antialiasing else kernel\n",
        "    kernel_width *= 1.0 / scale if antialiasing else 1.0\n",
        "\n",
        "    # These are the coordinates of the output image\n",
        "    out_coordinates = np.arange(1, out_length+1)\n",
        "\n",
        "    # These are the matching positions of the output-coordinates on the input image coordinates.\n",
        "    # Best explained by example: say we have 4 horizontal pixels for HR and we downscale by SF=2 and get 2 pixels:\n",
        "    # [1,2,3,4] -> [1,2]. Remember each pixel number is the middle of the pixel.\n",
        "    # The scaling is done between the distances and not pixel numbers (the right boundary of pixel 4 is transformed to\n",
        "    # the right boundary of pixel 2. pixel 1 in the small image matches the boundary between pixels 1 and 2 in the big\n",
        "    # one and not to pixel 2. This means the position is not just multiplication of the old pos by scale-factor).\n",
        "    # So if we measure distance from the left border, middle of pixel 1 is at distance d=0.5, border between 1 and 2 is\n",
        "    # at d=1, and so on (d = p - 0.5).  we calculate (d_new = d_old / sf) which means:\n",
        "    # (p_new-0.5 = (p_old-0.5) / sf)     ->          p_new = p_old/sf + 0.5 * (1-1/sf)\n",
        "    match_coordinates = 1.0 * out_coordinates / scale + 0.5 * (1 - 1.0 / scale)\n",
        "\n",
        "    # This is the left boundary to start multiplying the filter from, it depends on the size of the filter\n",
        "    left_boundary = np.floor(match_coordinates - kernel_width / 2)\n",
        "\n",
        "    # Kernel width needs to be enlarged because when covering has sub-pixel borders, it must 'see' the pixel centers\n",
        "    # of the pixels it only covered a part from. So we add one pixel at each side to consider (weights can zeroize them)\n",
        "    expanded_kernel_width = np.ceil(kernel_width) + 2\n",
        "\n",
        "    # Determine a set of field_of_view for each each output position, these are the pixels in the input image\n",
        "    # that the pixel in the output image 'sees'. We get a matrix whos horizontal dim is the output pixels (big) and the\n",
        "    # vertical dim is the pixels it 'sees' (kernel_size + 2)\n",
        "    field_of_view = np.squeeze(np.uint(np.expand_dims(left_boundary, axis=1) + np.arange(expanded_kernel_width) - 1))\n",
        "\n",
        "    # Assign weight to each pixel in the field of view. A matrix whos horizontal dim is the output pixels and the\n",
        "    # vertical dim is a list of weights matching to the pixel in the field of view (that are specified in\n",
        "    # 'field_of_view')\n",
        "    weights = fixed_kernel(1.0 * np.expand_dims(match_coordinates, axis=1) - field_of_view - 1)\n",
        "\n",
        "    # Normalize weights to sum up to 1. be careful from dividing by 0\n",
        "    sum_weights = np.sum(weights, axis=1)\n",
        "    sum_weights[sum_weights == 0] = 1.0\n",
        "    weights = 1.0 * weights / np.expand_dims(sum_weights, axis=1)\n",
        "\n",
        "    # We use this mirror structure as a trick for reflection padding at the boundaries\n",
        "    mirror = np.uint(np.concatenate((np.arange(in_length), np.arange(in_length - 1, -1, step=-1))))\n",
        "    field_of_view = mirror[np.mod(field_of_view, mirror.shape[0])]\n",
        "\n",
        "    # Get rid of  weights and pixel positions that are of zero weight\n",
        "    non_zero_out_pixels = np.nonzero(np.any(weights, axis=0))\n",
        "    weights = np.squeeze(weights[:, non_zero_out_pixels])\n",
        "    field_of_view = np.squeeze(field_of_view[:, non_zero_out_pixels])\n",
        "\n",
        "    # Final products are the relative positions and the matching weights, both are output_size X fixed_kernel_size\n",
        "    return weights, field_of_view\n",
        "\n",
        "\n",
        "def resize_along_dim(im, dim, weights, field_of_view):\n",
        "    # To be able to act on each dim, we swap so that dim 0 is the wanted dim to resize\n",
        "    tmp_im = np.swapaxes(im, dim, 0)\n",
        "\n",
        "    # We add singleton dimensions to the weight matrix so we can multiply it with the big tensor we get for\n",
        "    # tmp_im[field_of_view.T], (bsxfun style)\n",
        "    weights = np.reshape(weights.T, list(weights.T.shape) + (np.ndim(im) - 1) * [1])\n",
        "\n",
        "    # This is a bit of a complicated multiplication: tmp_im[field_of_view.T] is a tensor of order image_dims+1.\n",
        "    # for each pixel in the output-image it matches the positions the influence it from the input image (along 1 dim\n",
        "    # only, this is why it only adds 1 dim to the shape). We then multiply, for each pixel, its set of positions with\n",
        "    # the matching set of weights. we do this by this big tensor element-wise multiplication (MATLAB bsxfun style:\n",
        "    # matching dims are multiplied element-wise while singletons mean that the matching dim is all multiplied by the\n",
        "    # same number\n",
        "    tmp_out_im = np.sum(tmp_im[field_of_view.T] * weights, axis=0)\n",
        "\n",
        "    # Finally we swap back the axes to the original order\n",
        "    return np.swapaxes(tmp_out_im, dim, 0)\n",
        "\n",
        "\n",
        "def numeric_kernel(im, kernel, scale_factor, output_shape, kernel_shift_flag):\n",
        "    # See kernel_shift function to understand what this is\n",
        "    if kernel_shift_flag:\n",
        "        kernel = kernel_shift(kernel, scale_factor)\n",
        "\n",
        "    # First run a correlation (convolution with flipped kernel)\n",
        "    out_im = np.zeros_like(im)\n",
        "    for channel in range(np.ndim(im)):\n",
        "        out_im[:, :, channel] = filters.correlate(im[:, :, channel], kernel)\n",
        "\n",
        "    # Then subsample and return\n",
        "    return out_im[np.round(np.linspace(0, im.shape[0] - 1 / scale_factor[0], output_shape[0])).astype(int)[:, None],\n",
        "                  np.round(np.linspace(0, im.shape[1] - 1 / scale_factor[1], output_shape[1])).astype(int), :]\n",
        "\n",
        "\n",
        "def kernel_shift(kernel, sf):\n",
        "    # There are two reasons for shifting the kernel:\n",
        "    # 1. Center of mass is not in the center of the kernel which creates ambiguity. There is no possible way to know\n",
        "    #    the degradation process included shifting so we always assume center of mass is center of the kernel.\n",
        "    # 2. We further shift kernel center so that top left result pixel corresponds to the middle of the sfXsf first\n",
        "    #    pixels. Default is for odd size to be in the middle of the first pixel and for even sized kernel to be at the\n",
        "    #    top left corner of the first pixel. that is why different shift size needed between od and even size.\n",
        "    # Given that these two conditions are fulfilled, we are happy and aligned, the way to test it is as follows:\n",
        "    # The input image, when interpolated (regular bicubic) is exactly aligned with ground truth.\n",
        "\n",
        "    # First calculate the current center of mass for the kernel\n",
        "    current_center_of_mass = measurements.center_of_mass(kernel)\n",
        "\n",
        "    # The second (\"+ 0.5 * ....\") is for applying condition 2 from the comments above\n",
        "    wanted_center_of_mass = np.array(kernel.shape) / 2 + 0.5 * (sf - (kernel.shape[0] % 2))\n",
        "\n",
        "    # Define the shift vector for the kernel shifting (x,y)\n",
        "    shift_vec = wanted_center_of_mass - current_center_of_mass\n",
        "\n",
        "    # Before applying the shift, we first pad the kernel so that nothing is lost due to the shift\n",
        "    # (biggest shift among dims + 1 for safety)\n",
        "    kernel = np.pad(kernel, np.int(np.ceil(np.max(shift_vec))) + 1, 'constant')\n",
        "\n",
        "    # Finally shift the kernel and return\n",
        "    return interpolation.shift(kernel, shift_vec)\n",
        "\n",
        "\n",
        "# These next functions are all interpolation methods. x is the distance from the left pixel center\n",
        "\n",
        "\n",
        "def cubic(x):\n",
        "    absx = np.abs(x)\n",
        "    absx2 = absx ** 2\n",
        "    absx3 = absx ** 3\n",
        "    return ((1.5*absx3 - 2.5*absx2 + 1) * (absx <= 1) +\n",
        "            (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * ((1 < absx) & (absx <= 2)))\n",
        "\n",
        "\n",
        "def lanczos2(x):\n",
        "    return (((np.sin(pi*x) * np.sin(pi*x/2) + np.finfo(np.float32).eps) /\n",
        "             ((pi**2 * x**2 / 2) + np.finfo(np.float32).eps))\n",
        "            * (abs(x) < 2))\n",
        "\n",
        "\n",
        "def box(x):\n",
        "    return ((-0.5 <= x) & (x < 0.5)) * 1.0\n",
        "\n",
        "\n",
        "def lanczos3(x):\n",
        "    return (((np.sin(pi*x) * np.sin(pi*x/3) + np.finfo(np.float32).eps) /\n",
        "            ((pi**2 * x**2 / 3) + np.finfo(np.float32).eps))\n",
        "            * (abs(x) < 3))\n",
        "\n",
        "\n",
        "def linear(x):\n",
        "    return (x + 1) * ((-1 <= x) & (x < 0)) + (1 - x) * ((0 <= x) & (x <= 1))\n",
        "\n",
        "\n",
        "def np_imresize(im, scale_factor=None, output_shape=None, kernel=None, antialiasing=True, kernel_shift_flag=False):\n",
        "    return np.clip(imresize(im.transpose(1, 2, 0), scale_factor, output_shape, kernel, antialiasing,\n",
        "                            kernel_shift_flag).transpose(2, 0, 1), 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEKiz_dOutAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downsampler\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class Downsampler(nn.Module):\n",
        "    \"\"\"\n",
        "        http://www.realitypixels.com/turk/computergraphics/ResamplingFilters.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_planes, factor, kernel_type, phase=0, kernel_width=None, support=None, sigma=None,\n",
        "                 preserve_size=False):\n",
        "        \"\"\"\n",
        "        :param n_planes:\n",
        "        :param factor:\n",
        "        :param kernel_type:\n",
        "        :param float phase:\n",
        "        :param kernel_width:\n",
        "        :param support:\n",
        "        :param sigma:\n",
        "        :param preserve_size:\n",
        "        \"\"\"\n",
        "        super(Downsampler, self).__init__()\n",
        "\n",
        "        assert phase in [0, 0.5], 'phase should be 0 or 0.5'\n",
        "\n",
        "        if kernel_type == 'lanczos2':\n",
        "            support = 2\n",
        "            kernel_width = 4 * factor + 1\n",
        "            kernel_type_ = 'lanczos'\n",
        "\n",
        "        elif kernel_type == 'lanczos3':\n",
        "            support = 3\n",
        "            kernel_width = 6 * factor + 1\n",
        "            kernel_type_ = 'lanczos'\n",
        "\n",
        "        elif kernel_type == 'gauss12':\n",
        "            kernel_width = 7\n",
        "            sigma = 1 / 2\n",
        "            kernel_type_ = 'gauss'\n",
        "\n",
        "        elif kernel_type == 'gauss1sq2':\n",
        "            kernel_width = 9\n",
        "            sigma = 1. / np.sqrt(2)\n",
        "            kernel_type_ = 'gauss'\n",
        "\n",
        "        elif kernel_type in ['lanczos', 'gauss', 'box']:\n",
        "            kernel_type_ = kernel_type\n",
        "\n",
        "        else:\n",
        "            assert False, 'wrong name kernel'\n",
        "\n",
        "        # note that `kernel width` will be different to actual size for phase = 1/2\n",
        "        self.kernel = get_kernel(factor, kernel_type_, phase, kernel_width, support=support, sigma=sigma)\n",
        "\n",
        "        downsampler = nn.Conv2d(n_planes, n_planes, kernel_size=self.kernel.shape, stride=factor, padding=0)\n",
        "        downsampler.weight.data[:] = 0\n",
        "        downsampler.bias.data[:] = 0\n",
        "\n",
        "        kernel_torch = torch.from_numpy(self.kernel)\n",
        "        for i in range(n_planes):\n",
        "            downsampler.weight.data[i, i] = kernel_torch\n",
        "\n",
        "        self.downsampler_ = downsampler\n",
        "\n",
        "        if preserve_size:\n",
        "\n",
        "            if self.kernel.shape[0] % 2 == 1:\n",
        "                pad = int((self.kernel.shape[0] - 1) / 2.)\n",
        "            else:\n",
        "                pad = int((self.kernel.shape[0] - factor) / 2.)\n",
        "\n",
        "            self.padding = nn.ReplicationPad2d(pad)\n",
        "\n",
        "        self.preserve_size = preserve_size\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.preserve_size:\n",
        "            x = self.padding(input)\n",
        "        else:\n",
        "            x = input\n",
        "        self.x = x\n",
        "        return self.downsampler_(x)\n",
        "\n",
        "\n",
        "def get_kernel(factor, kernel_type, phase, kernel_width, support=None, sigma=None):\n",
        "    assert kernel_type in ['lanczos', 'gauss', 'box']\n",
        "\n",
        "    # factor  = float(factor)\n",
        "    if phase == 0.5 and kernel_type != 'box':\n",
        "        kernel = np.zeros([kernel_width - 1, kernel_width - 1])\n",
        "    else:\n",
        "        kernel = np.zeros([kernel_width, kernel_width])\n",
        "\n",
        "    if kernel_type == 'box':\n",
        "        assert phase == 0.5, 'Box filter is always half-phased'\n",
        "        kernel[:] = 1. / (kernel_width * kernel_width)\n",
        "\n",
        "    elif kernel_type == 'gauss':\n",
        "        assert sigma, 'sigma is not specified'\n",
        "        assert phase != 0.5, 'phase 1/2 for gauss not implemented'\n",
        "\n",
        "        center = (kernel_width + 1.) / 2.\n",
        "        print(center, kernel_width)\n",
        "        sigma_sq = sigma * sigma\n",
        "\n",
        "        for i in range(1, kernel.shape[0] + 1):\n",
        "            for j in range(1, kernel.shape[1] + 1):\n",
        "                di = (i - center) / 2.\n",
        "                dj = (j - center) / 2.\n",
        "                kernel[i - 1][j - 1] = np.exp(-(di * di + dj * dj) / (2 * sigma_sq))\n",
        "                kernel[i - 1][j - 1] = kernel[i - 1][j - 1] / (2. * np.pi * sigma_sq)\n",
        "    elif kernel_type == 'lanczos':\n",
        "        assert support, 'support is not specified'\n",
        "        center = (kernel_width + 1) / 2.\n",
        "\n",
        "        for i in range(1, kernel.shape[0] + 1):\n",
        "            for j in range(1, kernel.shape[1] + 1):\n",
        "\n",
        "                if phase == 0.5:\n",
        "                    di = abs(i + 0.5 - center) / factor\n",
        "                    dj = abs(j + 0.5 - center) / factor\n",
        "                else:\n",
        "                    di = abs(i - center) / factor\n",
        "                    dj = abs(j - center) / factor\n",
        "\n",
        "                pi_sq = np.pi * np.pi\n",
        "\n",
        "                val = 1\n",
        "                if di != 0:\n",
        "                    val = val * support * np.sin(np.pi * di) * np.sin(np.pi * di / support)\n",
        "                    val = val / (np.pi * np.pi * di * di)\n",
        "\n",
        "                if dj != 0:\n",
        "                    val = val * support * np.sin(np.pi * dj) * np.sin(np.pi * dj / support)\n",
        "                    val = val / (np.pi * np.pi * dj * dj)\n",
        "\n",
        "                kernel[i - 1][j - 1] = val\n",
        "\n",
        "\n",
        "    else:\n",
        "        assert False, 'wrong method name'\n",
        "\n",
        "    kernel /= kernel.sum()\n",
        "\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def get_downsampled(image, downsample_factors):\n",
        "    \"\"\"\n",
        "    image is of type np.array\n",
        "    downsampling_factor should be integer - e.g. 2 \n",
        "    \"\"\"\n",
        "    # TODO: move kernel type to args\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    data_type = torch.cuda.FloatTensor\n",
        "    image_torch = np_to_torch(image).type(data_type)\n",
        "    downsampled_images = [image_torch]\n",
        "    for i in downsample_factors:\n",
        "        downsampler = Downsampler(n_planes=image_torch.shape[1], factor=i,\n",
        "                                  kernel_type='lanczos2', phase=0.5, preserve_size=True).cuda()\n",
        "        downsampled_images.append(downsampler(image_torch))\n",
        "\n",
        "    return [torch_to_np(crop_torch_image(image, d=32)) for image in downsampled_images]\n",
        "\n",
        "\n",
        "\n",
        "def get_imresize_downsampled(image, downsampling_factor, downsampling_number):\n",
        "    \"\"\"\n",
        "    image is of type np.array\n",
        "    downsampling_factor should be integer - e.g. 0.5\n",
        "    \"\"\"\n",
        "    # TODO: move kernel type to args\n",
        "    downsampled_images = [image]\n",
        "    for i in range(1, downsampling_number + 1):\n",
        "        im = np.clip(imresize(image.transpose(1, 2, 0), scale_factor=(1 - (downsampling_factor * downsampling_number))).transpose(2,0,1), 0,1)\n",
        "        downsampled_images.append(pil_to_np(crop_image(np_to_pil(im), d=32)))\n",
        "\n",
        "    return downsampled_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT8NnHFPvNyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Layers.py\n",
        "def weights_init(m):\n",
        "    \"\"\" This is used to initialize weights of any network \"\"\"\n",
        "    class_name = m.__class__.__name__\n",
        "    if class_name.find('Conv') != -1:\n",
        "        nn.init.xavier_normal(m.weight, 1.0)\n",
        "        if hasattr(m.bias, 'data'):\n",
        "            m.bias.data.fill_(0)\n",
        "    elif class_name.find('BatchNorm2d') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Ratio(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Ratio, self).__init__()\n",
        "        self.multp = torch.autograd.Variable(torch.tensor(np.random.uniform(0, 1)),\n",
        "                                             requires_grad=True).type(torch.cuda.FloatTensor)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self):\n",
        "        return self.sig(self.multp)\n",
        "\n",
        "\n",
        "class VectorRatio(nn.Module):\n",
        "    def __init__(self, frames_number):\n",
        "        super(VectorRatio, self).__init__()\n",
        "        self.multp = torch.autograd.Variable(\n",
        "            torch.tensor(np.random.uniform(0, 1, frames_number)).reshape([frames_number, 1, 1, 1])).type(torch.cuda.FloatTensor)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self):\n",
        "        return self.sig(self.multp)\n",
        "\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def __init__(self, dim, *args):\n",
        "        super(Concat, self).__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        for idx, module_ in enumerate(args):\n",
        "            self.add_module(str(idx), module_)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        inputs = []\n",
        "        for module_ in self._modules.values():\n",
        "            inputs.append(module_(input_))\n",
        "\n",
        "        inputs_shapes2 = [x.shape[2] for x in inputs]\n",
        "        inputs_shapes3 = [x.shape[3] for x in inputs]\n",
        "\n",
        "        if np.all(np.array(inputs_shapes2) == min(inputs_shapes2)) and np.all(\n",
        "                        np.array(inputs_shapes3) == min(inputs_shapes3)):\n",
        "            inputs_ = inputs\n",
        "        else:\n",
        "            target_shape2 = min(inputs_shapes2)\n",
        "            target_shape3 = min(inputs_shapes3)\n",
        "\n",
        "            inputs_ = []\n",
        "            for inp in inputs:\n",
        "                diff2 = (inp.size(2) - target_shape2) // 2\n",
        "                diff3 = (inp.size(3) - target_shape3) // 2\n",
        "                inputs_.append(inp[:, :, diff2: diff2 + target_shape2, diff3:diff3 + target_shape3])\n",
        "\n",
        "        return torch.cat(inputs_, dim=self.dim)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._modules)\n",
        "\n",
        "\n",
        "class GenNoise(nn.Module):\n",
        "    def __init__(self, dim2):\n",
        "        super(GenNoise, self).__init__()\n",
        "        self.dim2 = dim2\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = list(x.size())\n",
        "        a[1] = self.dim2\n",
        "\n",
        "        b = torch.zeros(a).type_as(x.data)\n",
        "        b.normal_()\n",
        "\n",
        "        x = torch.autograd.Variable(b)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"\n",
        "        https://arxiv.org/abs/1710.05941\n",
        "        The hype was so huge that I could not help but try it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Swish, self).__init__()\n",
        "        self.s = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.s(x)\n",
        "\n",
        "\n",
        "def act(act_fun='LeakyReLU'):\n",
        "    \"\"\"\n",
        "        Either string defining an activation function or module (e.g. nn.ReLU)\n",
        "    \"\"\"\n",
        "    if isinstance(act_fun, str):\n",
        "        if act_fun == 'LeakyReLU':\n",
        "            return nn.LeakyReLU(0.2, inplace=True)\n",
        "        elif act_fun == 'Swish':\n",
        "            return Swish()\n",
        "        elif act_fun == 'ELU':\n",
        "            return nn.ELU()\n",
        "        elif act_fun == 'none':\n",
        "            return nn.Sequential()\n",
        "        else:\n",
        "            assert False\n",
        "    else:\n",
        "        return act_fun()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def bn(num_features):\n",
        "    return nn.BatchNorm2d(num_features)\n",
        "\n",
        "\n",
        "def conv(in_f, out_f, kernel_size, stride=1, bias=True, pad='zero', downsample_mode='stride'):\n",
        "    downsampler = None\n",
        "    if stride != 1 and downsample_mode != 'stride':\n",
        "\n",
        "        if downsample_mode == 'avg':\n",
        "            downsampler = nn.AvgPool2d(stride, stride)\n",
        "        elif downsample_mode == 'max':\n",
        "            downsampler = nn.MaxPool2d(stride, stride)\n",
        "        elif downsample_mode in ['lanczos2', 'lanczos3']:\n",
        "            downsampler = Downsampler(n_planes=out_f, factor=stride, kernel_type=downsample_mode, phase=0.5,\n",
        "                                      preserve_size=True)\n",
        "        stride = 1\n",
        "\n",
        "    padder = None\n",
        "    to_pad = int((kernel_size - 1) / 2)\n",
        "    if pad == 'reflection':\n",
        "        padder = nn.ReflectionPad2d(to_pad)\n",
        "        to_pad = 0\n",
        "\n",
        "    convolver = nn.Conv2d(in_f, out_f, kernel_size, stride, padding=to_pad, bias=bias)\n",
        "\n",
        "    layers = [x for x in [padder, convolver, downsampler] if x is not None]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FixedBlurLayer(nn.Module):\n",
        "    def __init__(self, kernel):\n",
        "        super(FixedBlurLayer, self).__init__()\n",
        "        self.kernel = kernel\n",
        "        to_pad_x = int((self.kernel.shape[0] - 1) / 2)\n",
        "        to_pad_y = int((self.kernel.shape[1] - 1) / 2)\n",
        "        self.pad = nn.ReflectionPad2d((to_pad_x, to_pad_x, to_pad_y, to_pad_y))\n",
        "        self.mask_np = np.zeros(shape=(1, 3, self.kernel.shape[0], self.kernel.shape[1]))\n",
        "        self.mask_np[0, 0, :, :] = self.kernel\n",
        "        self.mask_np[0, 1, :, :] = self.kernel\n",
        "        self.mask_np[0, 2, :, :] = self.kernel\n",
        "        self.mask = nn.Parameter(data=torch.cuda.FloatTensor(self.mask_np), requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(self.pad(x), self.mask)\n",
        "\n",
        "\n",
        "class VarianceLayer(nn.Module):\n",
        "    # TODO: make it pad-able\n",
        "    def __init__(self, patch_size=5, channels=1):\n",
        "        self.patch_size = patch_size\n",
        "        super(VarianceLayer, self).__init__()\n",
        "        mean_mask = np.ones((channels, channels, patch_size, patch_size)) / (patch_size * patch_size)\n",
        "        self.mean_mask = nn.Parameter(data=torch.cuda.FloatTensor(mean_mask), requires_grad=False)\n",
        "        mask = np.zeros((channels, channels, patch_size, patch_size))\n",
        "        mask[:, :, patch_size // 2, patch_size // 2] = 1.\n",
        "        self.ones_mask = nn.Parameter(data=torch.cuda.FloatTensor(mask), requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Ex_E = F.conv2d(x, self.ones_mask) - F.conv2d(x, self.mean_mask)\n",
        "        return F.conv2d((Ex_E) ** 2, self.mean_mask)\n",
        "\n",
        "\n",
        "class CovarianceLayer(nn.Module):\n",
        "    def __init__(self, patch_size=5, channels=1):\n",
        "        self.patch_size = patch_size\n",
        "        super(CovarianceLayer, self).__init__()\n",
        "        mean_mask = np.ones((channels, channels, patch_size, patch_size)) / (patch_size * patch_size)\n",
        "        self.mean_mask = nn.Parameter(data=torch.cuda.FloatTensor(mean_mask), requires_grad=False)\n",
        "        mask = np.zeros((channels, channels, patch_size, patch_size))\n",
        "        mask[:, :, patch_size // 2, patch_size // 2] = 1.\n",
        "        self.ones_mask = nn.Parameter(data=torch.cuda.FloatTensor(mask), requires_grad=False)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return F.conv2d((F.conv2d(x, self.ones_mask) - F.conv2d(x, self.mean_mask)) *\n",
        "                        (F.conv2d(y, self.ones_mask) - F.conv2d(y, self.mean_mask)), self.mean_mask)\n",
        "\n",
        "class GrayscaleLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GrayscaleLayer, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.mean(x, 1, keepdim=True)\n",
        "\n",
        "\n",
        "\n",
        "def add_module(self, module_):\n",
        "    self.add_module(str(len(self) + 1), module_)\n",
        "\n",
        "\n",
        "torch.nn.Module.add = add_module"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUMR3zvEvY7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Losses.py\n",
        "class StdLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Loss on the variance of the image.\n",
        "        Works in the grayscale.\n",
        "        If the image is smooth, gets zero\n",
        "        \"\"\"\n",
        "        super(StdLoss, self).__init__()\n",
        "        blur = (1 / 25) * np.ones((5, 5))\n",
        "        blur = blur.reshape(1, 1, blur.shape[0], blur.shape[1])\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.blur = nn.Parameter(data=torch.cuda.FloatTensor(blur), requires_grad=False)\n",
        "        image = np.zeros((5, 5))\n",
        "        image[2, 2] = 1\n",
        "        image = image.reshape(1, 1, image.shape[0], image.shape[1])\n",
        "        self.image = nn.Parameter(data=torch.cuda.FloatTensor(image), requires_grad=False)\n",
        "        self.gray_scale = GrayscaleLayer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gray_scale(x)\n",
        "        return self.mse(torch.nn.functional.conv2d(x, self.image), torch.nn.functional.conv2d(x, self.blur))\n",
        "\n",
        "\n",
        "class ExclusionLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, level=3):\n",
        "        \"\"\"\n",
        "        Loss on the gradient. based on:\n",
        "        http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single_Image_Reflection_CVPR_2018_paper.pdf\n",
        "        \"\"\"\n",
        "        super(ExclusionLoss, self).__init__()\n",
        "        self.level = level\n",
        "        self.avg_pool = torch.nn.AvgPool2d(2, stride=2).type(torch.cuda.FloatTensor)\n",
        "        self.sigmoid = nn.Sigmoid().type(torch.cuda.FloatTensor)\n",
        "\n",
        "    def get_gradients(self, img1, img2):\n",
        "        gradx_loss = []\n",
        "        grady_loss = []\n",
        "\n",
        "        for l in range(self.level):\n",
        "            gradx1, grady1 = self.compute_gradient(img1)\n",
        "            gradx2, grady2 = self.compute_gradient(img2)\n",
        "            # alphax = 2.0 * torch.mean(torch.abs(gradx1)) / torch.mean(torch.abs(gradx2))\n",
        "            # alphay = 2.0 * torch.mean(torch.abs(grady1)) / torch.mean(torch.abs(grady2))\n",
        "            alphay = 1\n",
        "            alphax = 1\n",
        "            gradx1_s = (self.sigmoid(gradx1) * 2) - 1\n",
        "            grady1_s = (self.sigmoid(grady1) * 2) - 1\n",
        "            gradx2_s = (self.sigmoid(gradx2 * alphax) * 2) - 1\n",
        "            grady2_s = (self.sigmoid(grady2 * alphay) * 2) - 1\n",
        "\n",
        "            # gradx_loss.append(torch.mean(((gradx1_s ** 2) * (gradx2_s ** 2))) ** 0.25)\n",
        "            # grady_loss.append(torch.mean(((grady1_s ** 2) * (grady2_s ** 2))) ** 0.25)\n",
        "            gradx_loss += self._all_comb(gradx1_s, gradx2_s)\n",
        "            grady_loss += self._all_comb(grady1_s, grady2_s)\n",
        "            img1 = self.avg_pool(img1)\n",
        "            img2 = self.avg_pool(img2)\n",
        "        return gradx_loss, grady_loss\n",
        "\n",
        "    def _all_comb(self, grad1_s, grad2_s):\n",
        "        v = []\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                v.append(torch.mean(((grad1_s[:, j, :, :] ** 2) * (grad2_s[:, i, :, :] ** 2))) ** 0.25)\n",
        "        return v\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        gradx_loss, grady_loss = self.get_gradients(img1, img2)\n",
        "        loss_gradxy = sum(gradx_loss) / (self.level * 9) + sum(grady_loss) / (self.level * 9)\n",
        "        return loss_gradxy / 2.0\n",
        "\n",
        "    def compute_gradient(self, img):\n",
        "        gradx = img[:, :, 1:, :] - img[:, :, :-1, :]\n",
        "        grady = img[:, :, :, 1:] - img[:, :, :, :-1]\n",
        "        return gradx, grady\n",
        "\n",
        "\n",
        "class ExtendedL1Loss(nn.Module):\n",
        "    \"\"\"\n",
        "    also pays attention to the mask, to be relative to its size\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ExtendedL1Loss, self).__init__()\n",
        "        self.l1 = nn.L1Loss().cuda()\n",
        "\n",
        "    def forward(self, a, b, mask):\n",
        "        normalizer = self.l1(mask, torch.zeros(mask.shape).cuda())\n",
        "        # if normalizer < 0.1:\n",
        "        #     normalizer = 0.1\n",
        "        c = self.l1(mask * a, mask * b) / normalizer\n",
        "        return c\n",
        "\n",
        "\n",
        "class NonBlurryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Loss on the distance to 0.5\n",
        "        \"\"\"\n",
        "        super(NonBlurryLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 1 - self.mse(x, torch.ones_like(x) * 0.5)\n",
        "\n",
        "\n",
        "class GrayscaleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GrayscaleLoss, self).__init__()\n",
        "        self.gray_scale = GrayscaleLayer()\n",
        "        self.mse = nn.MSELoss().cuda()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_g = self.gray_scale(x)\n",
        "        y_g = self.gray_scale(y)\n",
        "        return self.mse(x_g, y_g)\n",
        "\n",
        "\n",
        "class GrayLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GrayLoss, self).__init__()\n",
        "        self.l1 = nn.L1Loss().cuda()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = torch.ones_like(x) / 2.\n",
        "        return 1 / self.l1(x, y)\n",
        "\n",
        "\n",
        "class GradientLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 loss on the gradient of the picture\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GradientLoss, self).__init__()\n",
        "\n",
        "    def forward(self, a):\n",
        "        gradient_a_x = torch.abs(a[:, :, :, :-1] - a[:, :, :, 1:])\n",
        "        gradient_a_y = torch.abs(a[:, :, :-1, :] - a[:, :, 1:, :])\n",
        "        return torch.mean(gradient_a_x) + torch.mean(gradient_a_y)\n",
        "\n",
        "\n",
        "class YIQGNGCLoss(nn.Module):\n",
        "    def __init__(self, shape=5):\n",
        "        super(YIQGNGCLoss, self).__init__()\n",
        "        self.shape = shape\n",
        "        self.var = VarianceLayer(self.shape, channels=1)\n",
        "        self.covar = CovarianceLayer(self.shape, channels=1)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        if x.shape[1] == 3:\n",
        "            x_g = rgb_to_yiq(x)[:, :1, :, :]  # take the Y part\n",
        "            y_g = rgb_to_yiq(y)[:, :1, :, :]  # take the Y part\n",
        "        else:\n",
        "            assert x.shape[1] == 1\n",
        "            x_g = x  # take the Y part\n",
        "            y_g = y  # take the Y part\n",
        "        c = torch.mean(self.covar(x_g, y_g) ** 2)\n",
        "        vv = torch.mean(self.var(x_g) * self.var(y_g))\n",
        "        return c / vv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQX5OX-4vl-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Noise.py\n",
        "def get_noise(input_depth, method, spatial_size, noise_type='u', var=1. / 100):\n",
        "    \"\"\"\n",
        "    Returns a pytorch.Tensor of size (1 x `input_depth` x `spatial_size[0]` x `spatial_size[1]`)\n",
        "    initialized in a specific way.\n",
        "    Args:\n",
        "        input_depth: number of channels in the tensor\n",
        "        method: `noise` for fillting tensor with noise; `meshgrid` for np.meshgrid\n",
        "        spatial_size: spatial size of the tensor to initialize\n",
        "        noise_type: 'u' for uniform; 'n' for normal\n",
        "        var: a factor, a noise will be multiplicated by. Basically it is standard deviation scaler.\n",
        "    \"\"\"\n",
        "    if isinstance(spatial_size, int):\n",
        "        spatial_size = (spatial_size, spatial_size)\n",
        "    if method == 'noise':\n",
        "        shape = [1, input_depth, spatial_size[0], spatial_size[1]]\n",
        "        net_input = torch.zeros(shape)\n",
        "\n",
        "        fill_noise(net_input, noise_type)\n",
        "        net_input *= var\n",
        "    elif method == 'meshgrid':\n",
        "        assert input_depth % 2 == 0\n",
        "        X, Y = np.meshgrid(np.arange(0, spatial_size[1]) / float(spatial_size[1] - 1),\n",
        "                           np.arange(0, spatial_size[0]) / float(spatial_size[0] - 1))\n",
        "        meshgrid = np.concatenate([X[None, :], Y[None, :]] * (input_depth // 2))\n",
        "        net_input = np_to_torch(meshgrid)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    return net_input\n",
        "\n",
        "\n",
        "def get_video_noise(input_depth, method, temporal_size, spatial_size, noise_type='u', var=1. / 100, type=\"dependant\"):\n",
        "    \"\"\"\n",
        "    Returns a pytorch.Tensor of size (frame_number x `input_depth` x `spatial_size[0]` x `spatial_size[1]`)\n",
        "    initialized in a specific way.\n",
        "    Args:\n",
        "        input_depth: number of channels in the tensor\n",
        "        method: `noise` for fillting tensor with noise; `meshgrid` for np.meshgrid\n",
        "        temporal_size: number of frames\n",
        "        spatial_size: spatial size of the tensor to initialize\n",
        "        noise_type: 'u' for uniform; 'n' for normal\n",
        "        var: a factor, a noise will be multiplicated by. Basically it is standard deviation scaler.\n",
        "    \"\"\"\n",
        "    if isinstance(spatial_size, int):\n",
        "        spatial_size = (spatial_size, spatial_size)\n",
        "    if method == 'noise':\n",
        "        all_noise = []\n",
        "        for i in range(temporal_size):\n",
        "            shape = [input_depth, spatial_size[0], spatial_size[1]]\n",
        "            if len(all_noise) > 0:\n",
        "                if type == \"dependant\":\n",
        "                    frame = np.random.uniform(0, 1, size=shape)\n",
        "                    frame *= var\n",
        "                    all_noise.append(all_noise[-1] + frame)\n",
        "                elif type == \"half_dependant\":\n",
        "                    frame = np.random.uniform(0, 1, size=shape)\n",
        "                    frame *= var\n",
        "                    new_noise = (all_noise[-1] + frame)\n",
        "                    new_noise[:input_depth // 2,:,:] = (var * 10) * np.random.uniform(0, 1, size=shape)[:input_depth // 2,:,:]\n",
        "                    all_noise.append(new_noise)\n",
        "            else:\n",
        "                frame = np.random.uniform(-0.5, 0.5, size=shape)\n",
        "                frame *= (var * 10)\n",
        "                all_noise.append(frame)\n",
        "        return np_to_torch(np.array(all_noise))[0]\n",
        "    elif method == 'meshgrid':\n",
        "        assert False\n",
        "        assert input_depth % 2 == 0\n",
        "        X, Y = np.meshgrid(np.arange(0, spatial_size[1]) / float(spatial_size[1] - 1),\n",
        "                           np.arange(0, spatial_size[0]) / float(spatial_size[0] - 1))\n",
        "        meshgrid = np.concatenate([X[None, :], Y[None, :]] * (input_depth // 2))\n",
        "        net_input = np_to_torch(meshgrid)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "    return net_input\n",
        "\n",
        "\n",
        "class NoiseNet(nn.Module):\n",
        "    def __init__(self, channels=3, kernel_size=5):\n",
        "        super(NoiseNet, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.channels = channels\n",
        "        to_pad = int((self.kernel_size - 1) / 2)\n",
        "        self.padder = nn.ReflectionPad2d(to_pad).type(torch.cuda.FloatTensor)\n",
        "        to_pad = 0\n",
        "        self.convolver = nn.Conv2d(channels, channels, self.kernel_size, 1, padding=to_pad, bias=True).type(torch.cuda.FloatTensor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels, (x.shape, self.channels)\n",
        "        first = F.relu(self.convolver(self.padder(x)))\n",
        "        second = F.relu(self.convolver(self.padder(first)))\n",
        "        third = F.relu(self.convolver(self.padder(second)))\n",
        "        assert x.shape == third.shape, (x.shape, third.shape)\n",
        "        return third\n",
        "\n",
        "\n",
        "def fill_noise(x, noise_type):\n",
        "    \"\"\"\n",
        "    Fills tensor `x` with noise of type `noise_type`.\n",
        "    \"\"\"\n",
        "    if noise_type == 'u':\n",
        "        x.uniform_(-0.5, 0.5)\n",
        "    elif noise_type == 'n':\n",
        "        x.normal_()\n",
        "    else:\n",
        "        assert False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFY2asJ_vzvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimization.py\n",
        "def optimize(optimizer_type, parameters, optimization_closure,\n",
        "             plot_closure,\n",
        "             learning_rate,\n",
        "             num_iter,\n",
        "             optimization_closure_args,\n",
        "             plot_closure_args):\n",
        "    \"\"\"\n",
        "    Runs optimization loop.\n",
        "    :param optimizer_type: 'LBFGS' of 'adam'\n",
        "    :param parameters: list of Tensors to optimize over\n",
        "    :param optimization_closure: function, that returns loss variable\n",
        "    :param plot_closure: function that plots the loss and other information\n",
        "    :param learning_rate: learning rate\n",
        "    :param num_iter: number of iterations\n",
        "    :param dict optimization_closure_args: the arguments for the optimization closure\n",
        "    :param dict plot_closure_args: the arguments for the plot closure\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if optimizer_type == 'LBFGS':\n",
        "        assert False\n",
        "\n",
        "    elif optimizer_type == 'adam':\n",
        "        print('Starting optimization with ADAM')\n",
        "        optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
        "\n",
        "        for j in range(num_iter):\n",
        "            optimizer.zero_grad()\n",
        "            optimization_results = optimization_closure(j, **optimization_closure_args)\n",
        "            if plot_closure:\n",
        "                plot_closure(j, *optimization_results, **plot_closure_args)\n",
        "            optimizer.step()\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "def uneven_optimize(optimizer_type, parameters, optimization_closure, \n",
        "                    plot_closure,\n",
        "                    learning_rate,\n",
        "                    num_iter, step,\n",
        "                    optimization_closure_args,\n",
        "                    plot_closure_args):\n",
        "    \"\"\"\n",
        "    Runs optimization loop.\n",
        "    :param optimizer_type: 'LBFGS' of 'adam'\n",
        "    :param parameters: list of Tensors to optimize over\n",
        "    :param optimization_closure: function, that returns loss variable\n",
        "    :param plot_closure: function that plots the loss and other information\n",
        "    :param learning_rate: learning rate\n",
        "    :param num_iter: number of iterations\n",
        "    :param dict optimization_closure_args: the arguments for the optimization closure\n",
        "    :param dict plot_closure_args: the arguments for the plot closure\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if optimizer_type == 'LBFGS':\n",
        "        assert False\n",
        "\n",
        "    elif optimizer_type == 'adam':\n",
        "        print('Starting optimization with ADAM')\n",
        "        next_step_optimization_args = None\n",
        "        for j in range(num_iter // step):\n",
        "            optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
        "            for i in range(step):\n",
        "                optimizer.zero_grad()\n",
        "                optimization_results, next_step_optimization_args_temp = \\\n",
        "                    optimization_closure(j*step + i, next_step_optimization_args, **optimization_closure_args)\n",
        "                if plot_closure:\n",
        "                    plot_closure(j*step + i, *optimization_results, **plot_closure_args)\n",
        "                optimizer.step()\n",
        "                if next_step_optimization_args is None:\n",
        "                    # step zero\n",
        "                    next_step_optimization_args = next_step_optimization_args_temp\n",
        "            next_step_optimization_args = next_step_optimization_args_temp\n",
        "    else:\n",
        "        assert False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnGzU43Mv7R_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# skip_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def skip(\n",
        "        num_input_channels=2, num_output_channels=3, num_channels_down=[16, 32, 64, 128, 128],\n",
        "        num_channels_up=[16, 32, 64, 128, 128],\n",
        "        num_channels_skip=[4, 4, 4, 4, 4], filter_size_down=3,\n",
        "        filter_size_up=3, filter_skip_size=1, need_sigmoid=True, need_bias=True,\n",
        "        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU', need1x1_up=True):\n",
        "    \"\"\"\n",
        "    Assembles encoder-decoder with skip connections.\n",
        "    Arguments:\n",
        "        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n",
        "        pad (string): zero|reflection (default: 'zero')\n",
        "        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n",
        "        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n",
        "    \"\"\"\n",
        "    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n",
        "\n",
        "    n_scales = len(num_channels_down)\n",
        "\n",
        "    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n",
        "        upsample_mode = [upsample_mode] * n_scales\n",
        "\n",
        "    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n",
        "        downsample_mode = [downsample_mode] * n_scales\n",
        "\n",
        "    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n",
        "        filter_size_down = [filter_size_down] * n_scales\n",
        "\n",
        "    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n",
        "        filter_size_up = [filter_size_up] * n_scales\n",
        "\n",
        "    last_scale = n_scales - 1\n",
        "\n",
        "    cur_depth = None\n",
        "\n",
        "    model = nn.Sequential()\n",
        "    model_tmp = model\n",
        "\n",
        "    input_depth = num_input_channels\n",
        "    for i in range(len(num_channels_down)):\n",
        "\n",
        "        deeper = nn.Sequential()\n",
        "        skip = nn.Sequential()\n",
        "\n",
        "        if num_channels_skip[i] != 0:\n",
        "            model_tmp.add(Concat(1, skip, deeper))\n",
        "        else:\n",
        "            model_tmp.add(deeper)\n",
        "\n",
        "        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n",
        "\n",
        "        if num_channels_skip[i] != 0:\n",
        "            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n",
        "            skip.add(bn(num_channels_skip[i]))\n",
        "            skip.add(act(act_fun))\n",
        "\n",
        "        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n",
        "                        downsample_mode=downsample_mode[i]))\n",
        "        deeper.add(bn(num_channels_down[i]))\n",
        "        deeper.add(act(act_fun))\n",
        "\n",
        "        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n",
        "        deeper.add(bn(num_channels_down[i]))\n",
        "        deeper.add(act(act_fun))\n",
        "\n",
        "        deeper_main = nn.Sequential()\n",
        "\n",
        "        if i == len(num_channels_down) - 1:\n",
        "            # The deepest\n",
        "            k = num_channels_down[i]\n",
        "        else:\n",
        "            deeper.add(deeper_main)\n",
        "            k = num_channels_up[i + 1]\n",
        "\n",
        "        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n",
        "\n",
        "        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n",
        "        model_tmp.add(bn(num_channels_up[i]))\n",
        "        # model_tmp.add(layer_norm(num_channels_up[i]))\n",
        "        model_tmp.add(act(act_fun))\n",
        "\n",
        "        if need1x1_up:\n",
        "            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n",
        "            model_tmp.add(bn(num_channels_up[i]))\n",
        "            # model_tmp.add(layer_norm(num_channels_up[i]))\n",
        "            model_tmp.add(act(act_fun))\n",
        "\n",
        "        input_depth = num_channels_down[i]\n",
        "        model_tmp = deeper_main\n",
        "\n",
        "    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n",
        "    if need_sigmoid:\n",
        "        model.add(nn.Sigmoid())\n",
        "    return model\n",
        "\n",
        "\n",
        "def skip_mask(\n",
        "        num_input_channels=2, num_output_channels=3,\n",
        "        num_channels_down=[16, 32, 64, 128, 128], num_channels_up=[16, 32, 64, 128, 128],\n",
        "        num_channels_skip=[4, 4, 4, 4, 4],\n",
        "        filter_size_down=3, filter_size_up=3, filter_skip_size=1,\n",
        "        need_sigmoid=True, need_bias=True,\n",
        "        pad='zero', upsample_mode='nearest', downsample_mode='stride', act_fun='LeakyReLU',\n",
        "        need1x1_up=True):\n",
        "    \"\"\"\n",
        "    Assembles encoder-decoder with skip connections.\n",
        "    Arguments:\n",
        "        act_fun: Either string 'LeakyReLU|Swish|ELU|none' or module (e.g. nn.ReLU)\n",
        "        pad (string): zero|reflection (default: 'zero')\n",
        "        upsample_mode (string): 'nearest|bilinear' (default: 'nearest')\n",
        "        downsample_mode (string): 'stride|avg|max|lanczos2' (default: 'stride')\n",
        "    \"\"\"\n",
        "    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n",
        "\n",
        "    n_scales = len(num_channels_down)\n",
        "\n",
        "    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n",
        "        upsample_mode = [upsample_mode] * n_scales\n",
        "\n",
        "    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n",
        "        downsample_mode = [downsample_mode] * n_scales\n",
        "\n",
        "    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n",
        "        filter_size_down = [filter_size_down] * n_scales\n",
        "\n",
        "    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n",
        "        filter_size_up = [filter_size_up] * n_scales\n",
        "\n",
        "    last_scale = n_scales - 1\n",
        "\n",
        "    cur_depth = None\n",
        "\n",
        "    model = nn.Sequential()\n",
        "    model_tmp = model\n",
        "\n",
        "    input_depth = num_input_channels\n",
        "    for i in range(len(num_channels_down)):\n",
        "\n",
        "        deeper = nn.Sequential()\n",
        "        skip = nn.Sequential()\n",
        "\n",
        "        if num_channels_skip[i] != 0:\n",
        "            model_tmp.add(Concat(1, skip, deeper))\n",
        "        else:\n",
        "            model_tmp.add(deeper)\n",
        "\n",
        "        model_tmp.add(bn(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i])))\n",
        "\n",
        "        if num_channels_skip[i] != 0:\n",
        "            skip.add(conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n",
        "            skip.add(bn(num_channels_skip[i]))\n",
        "            skip.add(act(act_fun))\n",
        "\n",
        "        # skip.add(Concat(2, GenNoise(nums_noise[i]), skip_part))\n",
        "\n",
        "        deeper.add(conv(input_depth, num_channels_down[i], filter_size_down[i], 2, bias=need_bias, pad=pad,\n",
        "                        downsample_mode=downsample_mode[i]))\n",
        "        deeper.add(bn(num_channels_down[i]))\n",
        "        deeper.add(act(act_fun))\n",
        "\n",
        "        deeper.add(conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n",
        "        deeper.add(bn(num_channels_down[i]))\n",
        "        deeper.add(act(act_fun))\n",
        "\n",
        "        deeper_main = nn.Sequential()\n",
        "\n",
        "        if i == len(num_channels_down) - 1:\n",
        "            # The deepest\n",
        "            k = num_channels_down[i]\n",
        "        else:\n",
        "            deeper.add(deeper_main)\n",
        "            k = num_channels_up[i + 1]\n",
        "\n",
        "        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i], align_corners=True))\n",
        "\n",
        "        model_tmp.add(conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad))\n",
        "        model_tmp.add(bn(num_channels_up[i]))\n",
        "        # model_tmp.add(layer_norm(num_channels_up[i]))\n",
        "        model_tmp.add(act(act_fun))\n",
        "\n",
        "        if need1x1_up:\n",
        "            model_tmp.add(conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n",
        "            model_tmp.add(bn(num_channels_up[i]))\n",
        "            model_tmp.add(act(act_fun))\n",
        "\n",
        "        input_depth = num_channels_down[i]\n",
        "        model_tmp = deeper_main\n",
        "\n",
        "    model.add(conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n",
        "    if need_sigmoid:\n",
        "        model.add(nn.Sigmoid())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5JZryipwHGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unet_model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x1 = self.inc(x)\n",
        "        self.x2 = self.down1(self.x1)\n",
        "        self.x3 = self.down2(self.x2)\n",
        "        self.x4 = self.down3(self.x3)\n",
        "        self.x5 = self.down4(self.x4)\n",
        "        self.x6 = self.up1(self.x5, self.x4)\n",
        "        self.x7 = self.up2(self.x6, self.x3)\n",
        "        self.x8 = self.up3(self.x7, self.x2)\n",
        "        self.x9 = self.up4(self.x8, self.x1)\n",
        "        self.y = self.outc(self.x9)\n",
        "        return self.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO26dUGCwNEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upsampler.py\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class UpsamplerModel(nn.Module):\n",
        "    def __init__(self, output_shape, factor):\n",
        "        assert output_shape[0] % factor == 0\n",
        "        assert output_shape[1] % factor == 0\n",
        "        super(UpsamplerModel, self).__init__()\n",
        "        self.output_shape = output_shape\n",
        "        seed = np.ones((1, 1, output_shape[0] // factor, output_shape[1] // factor)) * 0.5\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.seed = nn.Parameter(data=torch.cuda.FloatTensor(seed), requires_grad=True)\n",
        "\n",
        "    def forward(self):\n",
        "        return nn.functional.interpolate(self.sigmoid(self.seed), size=self.output_shape, mode='bilinear')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2PiQy8prc7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "from cv2.ximgproc import guidedFilter\n",
        "\n",
        "from skimage.measure import compare_psnr\n",
        "import torch.nn as nn\n",
        "import progressbar\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_dark_channel(image, w=15):\n",
        "    \"\"\"\n",
        "    Get the dark channel prior in the (RGB) image data.\n",
        "    Parameters\n",
        "    -----------\n",
        "    image:  an M * N * 3 numpy array containing data ([0, L-1]) in the image where\n",
        "        M is the height, N is the width, 3 represents R/G/B channels.\n",
        "    w:  window size\n",
        "    Return\n",
        "    -----------\n",
        "    An M * N array for the dark channel prior ([0, L-1]).\n",
        "    \"\"\"\n",
        "    M, N, _ = image.shape\n",
        "    padded = np.pad(image, ((w // 2, w // 2), (w // 2, w // 2), (0, 0)), 'edge')\n",
        "    darkch = np.zeros((M, N))\n",
        "    for i, j in np.ndindex(darkch.shape):\n",
        "        darkch[i, j] = np.min(padded[i:i + w, j:j + w, :])  # CVPR09, eq.5\n",
        "    return darkch\n",
        "\n",
        "\n",
        "def get_atmosphere(image, p=0.0001, w=15):\n",
        "    \"\"\"Get the atmosphere light in the (RGB) image data.\n",
        "    Parameters\n",
        "    -----------\n",
        "    image:      the 3 * M * N RGB image data ([0, L-1]) as numpy array\n",
        "    w:      window for dark channel\n",
        "    p:      percentage of pixels for estimating the atmosphere light\n",
        "    Return\n",
        "    -----------\n",
        "    A 3-element array containing atmosphere light ([0, L-1]) for each channel\n",
        "    \"\"\"\n",
        "    image = image.transpose(1, 2, 0)\n",
        "    # reference CVPR09, 4.4\n",
        "    darkch = get_dark_channel(image, w)\n",
        "    M, N = darkch.shape\n",
        "    flatI = image.reshape(M * N, 3)\n",
        "    flatdark = darkch.ravel()\n",
        "    searchidx = (-flatdark).argsort()[:int(M * N * p)]  # find top M * N * p indexes\n",
        "    # return the highest intensity for each channel\n",
        "    return np.max(flatI.take(searchidx, axis=0), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "DehazeResult = namedtuple(\"DehazeResult\", ['learned', 't', 'a', 'psnr'])\n",
        "\n",
        "\n",
        "class Dehaze(object):\n",
        "    def __init__(self, image_name, image, num_iter=8000, plot_during_training=True,\n",
        "                 show_every=500,\n",
        "                 use_deep_channel_prior=True,\n",
        "                 gt_ambient=None, clip=True):\n",
        "        self.image_name = image_name\n",
        "        self.image = image\n",
        "\n",
        "        self.num_iter = num_iter\n",
        "        self.plot_during_training = plot_during_training\n",
        "        self.show_every = show_every\n",
        "        self.use_deep_channel_prior = use_deep_channel_prior\n",
        "        self.gt_ambient = gt_ambient  # np\n",
        "        self.ambient_net = None\n",
        "        self.image_net = None\n",
        "        self.mask_net = None\n",
        "        self.ambient_val = None\n",
        "        self.mse_loss = None\n",
        "        self.learning_rate = 0.001\n",
        "        self.parameters = None\n",
        "        self.current_result = None\n",
        "\n",
        "        self.clip = clip\n",
        "        self.blur_loss = None\n",
        "        self.best_result = None\n",
        "        self.image_net_inputs = None\n",
        "        self.mask_net_inputs = None\n",
        "        self.image_out = None\n",
        "        self.mask_out = None\n",
        "        self.done = False\n",
        "        self.ambient_out = None\n",
        "        self.total_loss = None\n",
        "        self.input_depth = 8\n",
        "        self.post = None\n",
        "        self._init_all()\n",
        "\n",
        "    def _init_images(self):\n",
        "        self.original_image = self.image.copy()\n",
        "        factor = 1\n",
        "        image = self.image\n",
        "        while image.shape[1] >= 800 or image.shape[2] >= 800:\n",
        "            new_shape_x, new_shape_y = self.image.shape[1] / factor, self.image.shape[2] /factor\n",
        "            new_shape_x -= (new_shape_x % 32)\n",
        "            new_shape_y -= (new_shape_y % 32)\n",
        "            image = np_imresize(self.image, output_shape=(new_shape_x, new_shape_y))\n",
        "            factor += 1\n",
        "        self.images = create_augmentations(image)\n",
        "        self.images_torch = [np_to_torch(image).type(torch.cuda.FloatTensor) for image in self.images]\n",
        "\n",
        "    def _is_learning_ambient(self):\n",
        "        \"\"\"\n",
        "        true if the ambient is learned during the optimization process\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return not self.use_deep_channel_prior # and not isinstance(self.gt_ambient, np.ndarray)\n",
        "\n",
        "    def _init_nets(self):\n",
        "        input_depth = self.input_depth\n",
        "        data_type = torch.cuda.FloatTensor\n",
        "        pad = 'reflection'\n",
        "\n",
        "        image_net = skip(\n",
        "            input_depth, 3,\n",
        "            num_channels_down=[8, 16, 32, 64, 128],\n",
        "            num_channels_up=[8, 16, 32, 64, 128],\n",
        "            num_channels_skip=[0, 0, 0, 4, 4],\n",
        "            upsample_mode='bilinear',\n",
        "            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU')\n",
        "\n",
        "        self.image_net = image_net.type(data_type)\n",
        "\n",
        "        mask_net = skip(\n",
        "            input_depth, 1,\n",
        "            num_channels_down=[8, 16, 32, 64, 128],\n",
        "            num_channels_up=[8, 16, 32, 64, 128],\n",
        "            num_channels_skip=[0, 0, 0, 4, 4],\n",
        "            upsample_mode='bilinear',\n",
        "            need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU')\n",
        "\n",
        "        self.mask_net = mask_net.type(data_type)\n",
        "\n",
        "    def _init_ambient(self):\n",
        "        if self._is_learning_ambient():\n",
        "            ambient_net = skip(\n",
        "                self.input_depth, 3,\n",
        "                num_channels_down=[8, 16, 32, 64, 128],\n",
        "                num_channels_up=[8, 16, 32, 64, 128],\n",
        "                num_channels_skip=[0, 0, 0, 4, 4],\n",
        "                upsample_mode='bilinear',\n",
        "                filter_size_down=3,\n",
        "                filter_size_up=3,\n",
        "                need_sigmoid=True, need_bias=True, pad='reflection', act_fun='LeakyReLU')\n",
        "            self.ambient_net = ambient_net.type(torch.cuda.FloatTensor)\n",
        "        if isinstance(self.gt_ambient, np.ndarray):\n",
        "            atmosphere = self.gt_ambient\n",
        "        else:\n",
        "            # use_deep_channel_prior is True\n",
        "            atmosphere = get_atmosphere(self.image)\n",
        "        self.ambient_val = nn.Parameter(data=torch.cuda.FloatTensor(atmosphere.reshape((1, 3, 1, 1))),\n",
        "                                        requires_grad=False)\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        parameters = [p for p in self.image_net.parameters()] + \\\n",
        "                     [p for p in self.mask_net.parameters()]\n",
        "        if self._is_learning_ambient():\n",
        "            parameters += [p for p in self.ambient_net.parameters()]\n",
        "\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def _init_loss(self):\n",
        "        data_type = torch.cuda.FloatTensor\n",
        "        self.mse_loss = torch.nn.MSELoss().type(data_type)\n",
        "        self.blur_loss = StdLoss().type(data_type)\n",
        "\n",
        "    def _init_inputs(self):\n",
        "        original_noises = create_augmentations(torch_to_np(get_noise(self.input_depth, 'noise',\n",
        "                                                                     (self.images[0].shape[1], self.images[0].shape[2]),\n",
        "                                          var=1/10.).type(torch.cuda.FloatTensor).detach()))\n",
        "        self.image_net_inputs = [np_to_torch(original_noise).type(torch.cuda.FloatTensor).detach()\n",
        "                                 for original_noise in original_noises]\n",
        "\n",
        "        original_noises = create_augmentations(torch_to_np(get_noise(self.input_depth, 'noise',\n",
        "                                                                     (self.images[0].shape[1], self.images[0].shape[2]),\n",
        "                                                                     var=1 / 10.).type(\n",
        "            torch.cuda.FloatTensor).detach()))\n",
        "        self.mask_net_inputs = [np_to_torch(original_noise).type(torch.cuda.FloatTensor).detach()\n",
        "                                 for original_noise in original_noises]\n",
        "        if self._is_learning_ambient():\n",
        "            self.ambient_net_input = get_noise(self.input_depth, 'meshgrid',\n",
        "                                                           (self.images[0].shape[1], self.images[0].shape[2])\n",
        "                                                           ).type(torch.cuda.FloatTensor).detach()\n",
        "\n",
        "    def _init_all(self):\n",
        "        self._init_images()\n",
        "        self._init_nets()\n",
        "        self._init_ambient()\n",
        "        self._init_inputs()\n",
        "        self._init_parameters()\n",
        "        self._init_loss()\n",
        "\n",
        "    def optimize(self):\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.parameters, lr=self.learning_rate)\n",
        "        for j in range(self.num_iter):\n",
        "            optimizer.zero_grad()\n",
        "            self._optimization_closure(j)\n",
        "            self._obtain_current_result(j)\n",
        "            if self.plot_during_training:\n",
        "                self._plot_closure(j)\n",
        "            if self.done:\n",
        "                return\n",
        "            optimizer.step()\n",
        "\n",
        "    def _get_augmentation(self, iteration):\n",
        "        return 0\n",
        "        # if iteration % 4 in [1, 2,3]:\n",
        "        #     return 0\n",
        "        # iteration //= 2\n",
        "        # return iteration % 8\n",
        "\n",
        "    def _optimization_closure(self, step):\n",
        "        \"\"\"\n",
        "        :param step: the number of the iteration\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if step == self.num_iter - 1:\n",
        "            aug = 0\n",
        "            reg_std = 0\n",
        "        else:\n",
        "            aug = self._get_augmentation(step)\n",
        "            reg_std = 1 / 30.\n",
        "        image_net_input = self.image_net_inputs[aug] + (self.image_net_inputs[aug].clone().normal_() * reg_std)\n",
        "        self.image_out = self.image_net(image_net_input)\n",
        "\n",
        "        if isinstance(self.ambient_net, nn.Module):\n",
        "            ambient_net_input = self.ambient_net_input + (self.ambient_net_input.clone().normal_() * reg_std)\n",
        "            self.ambient_out = self.ambient_net(ambient_net_input)  #[:, :,\n",
        "                               # self.images[0].shape[1] // 2:self.images[0].shape[1] // 2 + 1,\n",
        "                               # self.images[0].shape[2] // 2:self.images[0].shape[2] // 2 + 1]\n",
        "            # self.ambient_out  = self.ambient_out * torch.ones_like(self.image_out)\n",
        "        else:\n",
        "            self.ambient_out = self.ambient_val\n",
        "        self.mask_out = self.mask_net(self.mask_net_inputs[aug])\n",
        "\n",
        "        self.blur_out = self.blur_loss(self.mask_out)\n",
        "        self.total_loss = self.mse_loss(self.mask_out * self.image_out + (1 - self.mask_out) * self.ambient_out,\n",
        "                                 self.images_torch[aug]) + 0.005 * self.blur_out\n",
        "        if self._is_learning_ambient():\n",
        "            self.total_loss += 0.1 * self.blur_loss(self.ambient_out)\n",
        "            if step < 1000:\n",
        "                self.total_loss += self.mse_loss(self.ambient_out, self.ambient_val * torch.ones_like(self.ambient_out))\n",
        "        self.total_loss.backward(retain_graph=True)\n",
        "\n",
        "\n",
        "    def _obtain_current_result(self, step):\n",
        "        if step % 8 == 0:\n",
        "            image_out_np = np.clip(torch_to_np(self.image_out), 0, 1)\n",
        "            mask_out_np = np.clip(torch_to_np(self.mask_out), 0, 1)\n",
        "            ambient_out_np = np.clip(torch_to_np(self.ambient_out), 0, 1)\n",
        "            psnr = compare_psnr(self.images[0], mask_out_np * image_out_np + (1 - mask_out_np) * ambient_out_np)\n",
        "            self.current_result = DehazeResult(learned=image_out_np, t=mask_out_np, a=ambient_out_np, psnr=psnr)\n",
        "            if self.best_result is None or self.best_result.psnr < self.current_result.psnr:\n",
        "                self.best_result = self.current_result\n",
        "\n",
        "    def _plot_closure(self, step):\n",
        "        \"\"\"\n",
        "         :param step: the number of the iteration\n",
        "         :return:\n",
        "         \"\"\"\n",
        "        print('Iteration %05d    Loss %f  %f current_psnr: %f max_psnr %f' % (step, self.total_loss.item(),\n",
        "                                                                              self.blur_out.item(),\n",
        "                                                                           self.current_result.psnr,\n",
        "                                                                           self.best_result.psnr), '\\r', end='')\n",
        "        if step % self.show_every == self.show_every - 1:\n",
        "            plot_image_grid(\"t_and_amb\", [ self.best_result.a * np.ones_like(self.best_result.learned), self.best_result.t])\n",
        "            # original_image = t*image + (1-t)*A\n",
        "            # image = (original_image - (1 - t) * A) * (1/t)\n",
        "            plot_image_grid(\"current_image\", [self.images[0], np.clip(self.best_result.learned, 0, 1)])\n",
        "\n",
        "    def finalize(self):\n",
        "        self.final_image = np_imresize(self.best_result.learned, output_shape=self.original_image.shape[1:])\n",
        "        self.final_t_map = np_imresize(self.best_result.t, output_shape=self.original_image.shape[1:])\n",
        "        self.final_a = np_imresize(self.best_result.a, output_shape=self.original_image.shape[1:])\n",
        "        mask_out_np = self.t_matting(self.final_t_map)\n",
        "        self.post = np.clip((self.original_image - ((1 - mask_out_np) * self.final_a)) / mask_out_np, 0, 1)\n",
        "        save_image(self.image_name + \"_original\", np.clip(self.original_image, 0, 1))\n",
        "        # save_image(self.image_name + \"_learned\", self.final_image)\n",
        "        save_image(self.image_name + \"_t\", mask_out_np)\n",
        "        save_image(self.image_name + \"_final\", self.post)\n",
        "        save_image(self.image_name + \"_a\", np.clip(self.final_a, 0, 1))\n",
        "\n",
        "    def t_matting(self, mask_out_np):\n",
        "        refine_t = guidedFilter(self.original_image.transpose(1, 2, 0).astype(np.float32),\n",
        "                                mask_out_np[0].astype(np.float32), 50, 1e-4)\n",
        "        if self.clip:\n",
        "            return np.array([np.clip(refine_t, 0.1, 1)])\n",
        "        else:\n",
        "            return np.array([np.clip(refine_t, 0, 1)])\n",
        "\n",
        "\n",
        "def dehaze(image_name, image, num_iter=4000, plot_during_training=True,\n",
        "           show_every=500,\n",
        "           use_deep_channel_prior=True,\n",
        "           gt_ambient=None):\n",
        "    dh = Dehaze(image_name + \"_0\", image, num_iter, plot_during_training, show_every, use_deep_channel_prior,\n",
        "                gt_ambient, clip=True)\n",
        "    dh.optimize()\n",
        "    dh.finalize()\n",
        "    if use_deep_channel_prior:\n",
        "        assert not gt_ambient\n",
        "        gt_ambient = dh.best_result.a\n",
        "        use_deep_channel_prior = False\n",
        "    for i in range(1):\n",
        "        assert dh.post.shape == image.shape, (dh.post.shape, image.shape)\n",
        "        dh = Dehaze(image_name + \"_{}\".format(i+1), dh.post, num_iter, plot_during_training, show_every,\n",
        "                    use_deep_channel_prior, gt_ambient, clip=True)\n",
        "        dh.optimize()\n",
        "        dh.finalize()\n",
        "    post = dh.post\n",
        "    t = np.array([np.mean((image - dh.final_a) / (post - dh.final_a), axis=0)])\n",
        "    save_image(image_name + \"_original\", np.clip(image, 0, 1))\n",
        "    return dh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RselGSFXwZO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# the gt_ambient is taken from Bahat's code (https://github.com/YuvalBahat/Dehazing-Airlight-estimation)\n",
        "# i = prepare_image(\"/content/drive/My Drive/data-science/deep-learning/double-dip/images/hongkong.png\")\n",
        "# dehaze(\"hongkong\", i, use_deep_channel_prior=False, gt_ambient=np.array([0.5600084 , 0.64564645, 0.72515032]))\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "# i = prepare_image(\"/content/drive/My Drive/Deep-learning/double-dip/images/dense-haze1.png\")\n",
        "# dh = dehaze(\"dense-haze\", i, use_deep_channel_prior=True)\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# gt_ambient=np.array([0.71863767, 0.70432067, 0.62480165]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSgVx0bb5TlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dh.best_result.psnr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDuFlOoBmmLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# start_time = time.time()\n",
        "# i = prepare_image(\"/content/drive/My Drive/Deep-learning/double-dip/images/dense-haze2.png\")\n",
        "# dh2 = dehaze(\"dense-haze2\", i, use_deep_channel_prior=False, gt_ambient=np.array([0.71863767, 0.70432067, 0.62480165]))\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiCkWjO4cdJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dh2.best_result.psnr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AoDkcS4OpO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "random_images = ['04_hazy.png', '06_hazy.png', '07_hazy.png', '13_hazy.png', '14_hazy.png']\n",
        "result_dict={}\n",
        "for x in random_images:\n",
        "  start_time = time.time()\n",
        "  imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/\"+x\n",
        "  i = prepare_image(imag_path)\n",
        "  dh = dehaze(\"dense-haze-{}\".format(x), i, use_deep_channel_prior=True)\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  print(\"--- %s  ---\" % (x))\n",
        "  print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "  result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhCMwlTOOwWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzwz0XTzeWRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "17cf3e08-6d60-4a85-e610-0daccb5c8ff6"
      },
      "source": [
        "\n",
        "start_time = time.time()\n",
        "imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/04_hazy.png\"\n",
        "i = prepare_image(imag_path)\n",
        "dh = dehaze(\"dense-haze-04_hazy\", i, use_deep_channel_prior=True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "# result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:258: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/measure/simple_metrics.py:65: UserWarning: Inputs have mismatched dtype.  Setting data_range based on im_true.\n",
            "  return peak_signal_noise_ratio(im_true, im_test, data_range=data_range)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 1095.1276514530182 seconds ---\n",
            "--- 40.869334066004285  ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dx8hYcdS4OJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cd2707e4-90ea-4578-b9d0-179ae671cabd"
      },
      "source": [
        "start_time = time.time()\n",
        "imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/06_hazy.png\"\n",
        "i = prepare_image(imag_path)\n",
        "dh = dehaze(\"dense-haze-06_hazy\", i, use_deep_channel_prior=True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "# result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:258: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/measure/simple_metrics.py:65: UserWarning: Inputs have mismatched dtype.  Setting data_range based on im_true.\n",
            "  return peak_signal_noise_ratio(im_true, im_test, data_range=data_range)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 1725.7841794490814 seconds ---\n",
            "--- 39.1909459136593  ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--3qZfVFS8GS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8d108212-4ef5-4b8c-cedb-f0d1fee3b6eb"
      },
      "source": [
        "start_time = time.time()\n",
        "imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/07_hazy.png\"\n",
        "i = prepare_image(imag_path)\n",
        "dh = dehaze(\"dense-haze-07_hazy\", i, use_deep_channel_prior=True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "# result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:258: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/measure/simple_metrics.py:65: UserWarning: Inputs have mismatched dtype.  Setting data_range based on im_true.\n",
            "  return peak_signal_noise_ratio(im_true, im_test, data_range=data_range)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 2872.5376076698303 seconds ---\n",
            "--- 38.68008749722877  ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGzHb-9DTCIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "51d9d178-6bb7-4aad-951e-1247ad6394e7"
      },
      "source": [
        "start_time = time.time()\n",
        "imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/13_hazy.png\"\n",
        "i = prepare_image(imag_path)\n",
        "dh = dehaze(\"dense-haze-13_hazy\", i, use_deep_channel_prior=True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "# result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:258: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/measure/simple_metrics.py:65: UserWarning: Inputs have mismatched dtype.  Setting data_range based on im_true.\n",
            "  return peak_signal_noise_ratio(im_true, im_test, data_range=data_range)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 3488.6670093536377 seconds ---\n",
            "--- 43.31917324072499  ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1z2Zq1TFHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2e9413d2-5949-422c-ddce-14e1f0cae500"
      },
      "source": [
        "start_time = time.time()\n",
        "imag_path = \"/content/drive/My Drive/Deep-learning/double-dip/images/14_hazy.png\"\n",
        "i = prepare_image(imag_path)\n",
        "dh = dehaze(\"dense-haze-14_hazy\", i, use_deep_channel_prior=True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"--- %s  ---\" % (dh.best_result.psnr))\n",
        "# result_dict[x] = dh.best_result.psnr"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:258: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/measure/simple_metrics.py:65: UserWarning: Inputs have mismatched dtype.  Setting data_range based on im_true.\n",
            "  return peak_signal_noise_ratio(im_true, im_test, data_range=data_range)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--- 1050.7818093299866 seconds ---\n",
            "--- 41.89225211135333  ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlzU5R4qzvUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6cUQOatThgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}